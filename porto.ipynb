{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from rgf.sklearn import RGFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Коэффициент Gini\n",
    "def GiniScore(y_actual, y_pred):\n",
    "    return 2*roc_auc_score(y_actual, y_pred)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========KNN==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считываем данные\n",
    "# Сразу преобразовываем -1 в NA\n",
    "df_train = pd.read_csv('/resources/data/driver/train.csv', index_col='id', na_values=-1)\n",
    "\n",
    "# Тест сортируем по индексу\n",
    "df_test = pd.read_csv('/resources/data/driver/test.csv', index_col='id', na_values=-1).sort_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['ps_car_14']=df_train[df_train['ps_car_14'].notnull()]['ps_car_14'].apply(lambda x: round(x*1000000))\n",
    "df_train['ps_reg_03']=df_train[df_train['ps_reg_03'].notnull()]['ps_reg_03'].apply(lambda x: round(x*1000000))\n",
    "\n",
    "df_test['ps_car_14']=df_test[df_test['ps_car_14'].notnull()]['ps_car_14'].apply(lambda x: round(x*1000000))\n",
    "df_test['ps_reg_03']=df_test[df_test['ps_reg_03'].notnull()]['ps_reg_03'].apply(lambda x: round(x*1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'ps_car_11' ps_car_12 содержит пять и одну записей с target = 0 в трейне. Удаляем их\n",
    "df_train.dropna(subset=['ps_car_11'], inplace=True)\n",
    "df_train.dropna(subset=['ps_car_12'], inplace=True)\n",
    "\n",
    "# Преобразуем в год выпуска\n",
    "df_train['ps_car_15'] = df_train['ps_car_15'].apply(lambda x: round(x**2))\n",
    "df_train['ps_car_15'] = df_train['ps_car_15'].astype(np.int8)\n",
    "\n",
    "df_test['ps_car_15'] = df_test['ps_car_15'].apply(lambda x: round(x**2))\n",
    "df_test['ps_car_15'] = df_test['ps_car_15'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Разделение на категориальные и числовые (надо подумать)\n",
    "num_cols = np.array(['ps_reg_01', 'ps_reg_02', \n",
    "                     'ps_car_12', 'ps_car_13', \n",
    "                     'ps_calc_01','ps_calc_02','ps_calc_03'])\n",
    "cat_cols = np.array(list(set(df_test.columns.values.tolist()) - set(num_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 44s, sys: 12.8 s, total: 7min 57s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### TRAIN ###\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Признаки с пропусками\n",
    "lst = ['ps_ind_05_cat', 'ps_reg_03', 'ps_car_14',\n",
    "       'ps_car_09_cat', 'ps_car_07_cat', 'ps_car_01_cat', 'ps_ind_02_cat',\n",
    "       'ps_ind_04_cat', 'ps_car_02_cat', 'ps_car_05_cat', 'ps_car_03_cat']\n",
    "# Для каждого признака\n",
    "for i in lst:\n",
    "    \n",
    "    # Сортируем по заполняемому признаку, чтобы NA были в начале\n",
    "    df_train.sort_values(by=i, na_position='first', inplace=True)\n",
    "\n",
    "    # Значимые признаки по которым будем заполнять. Создаем отдельный датафрейм\n",
    "    df_knn = df_train[['ps_car_13', 'ps_ind_03', 'ps_reg_01', 'ps_ind_15', i]]\n",
    "\n",
    "    # Нормализуем тренировочный набор, целевую переменную не трогаем\n",
    "    # Сначала обучаем скелер на всех train данных \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_knn[df_knn.columns[:4]])\n",
    "\n",
    "    # Затем разбиваем на два датафрейма - с пропущенными и с заполненными значениями\n",
    "    # Плюс целевую переменную делаем отдельно как У\n",
    "    knn_x_train = df_knn.dropna(subset=[i]).drop([i], axis=1)\n",
    "    knn_y_train = df_knn.dropna(subset=[i])[i]\n",
    "    knn_x_test = df_knn[df_knn[i].isnull()].drop([i], axis=1)\n",
    "\n",
    "    # Нормализуем данные обученным скелером\n",
    "    knn_x_train = scaler.transform(knn_x_train)\n",
    "    knn_x_test = scaler.transform(knn_x_test)\n",
    "\n",
    "    # Прогнозируем пропущенные значения\n",
    "    if i in num_cols:\n",
    "        clf = KNeighborsRegressor(5, weights='distance')\n",
    "    else:\n",
    "        clf = KNeighborsClassifier(5, weights='distance')\n",
    "    clf.fit(knn_x_train, knn_y_train)\n",
    "    imputed_values = clf.predict(knn_x_test)\n",
    "\n",
    "    # Объединяем спрогнозированные и остальные\n",
    "    # И собираем таблицу в исходное состояние\n",
    "    # Теперь у нас вместо пропущенных значений - спрогнозированные\n",
    "    X2 = np.hstack((knn_x_test, imputed_values.reshape(-1,1)))\n",
    "    X1 = np.hstack((knn_x_train, knn_y_train[:, np.newaxis]))\n",
    "    df_knn = np.vstack((X2, X1))\n",
    "\n",
    "    # Помещаем заполненный столбец в исходную таблицу\n",
    "    # Меняем тип данных на нужный\n",
    "    # И сортируем по индексу - Готово!\n",
    "    df_train[i] = df_knn[:,-1]\n",
    "\n",
    "df_train.sort_index(inplace=True)\n",
    "\n",
    "### TEST ###\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "lst = ['ps_ind_05_cat', 'ps_reg_03', 'ps_car_14',\n",
    "       'ps_car_09_cat', 'ps_car_07_cat', 'ps_car_01_cat', 'ps_ind_02_cat',\n",
    "       'ps_ind_04_cat', 'ps_car_02_cat', 'ps_car_11', 'ps_car_05_cat', 'ps_car_03_cat']\n",
    "\n",
    "for i in lst:\n",
    "    # Сортируем по заполняемому признаку, чтобы NA были в верхней части таблицы\n",
    "    df_test.sort_values(by=i, na_position='first', inplace=True)\n",
    "\n",
    "    # Значимые признаки по которым будем заполнять. Создаем отдельную таблицу\n",
    "    df_knn = df_test[['ps_car_13', 'ps_ind_03', 'ps_reg_01', 'ps_ind_15', i]]\n",
    "\n",
    "    # Нормализуем тренировочный набор, целевую переменную не трогаем\n",
    "    # Сначала обучаем скелер на всех данных \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_knn[df_knn.columns[:4]])\n",
    "\n",
    "    # Затем разбиваем на две таблицы - с пропущенными и с заполненными значениями\n",
    "    # Плюс целевую переменную делаем отдельно как У\n",
    "    knn_x_train = df_knn.dropna(subset=[i]).drop([i], axis=1)\n",
    "    knn_y_train = df_knn.dropna(subset=[i])[i]\n",
    "    knn_x_test = df_knn[df_knn[i].isnull()].drop([i], axis=1)\n",
    "\n",
    "    # Нормализуем данные обученным скелером\n",
    "    knn_x_train = scaler.transform(knn_x_train)\n",
    "    knn_x_test = scaler.transform(knn_x_test)\n",
    "\n",
    "    # Прогнозируем пропущенные значения\n",
    "    if i in num_cols:\n",
    "        clf = KNeighborsRegressor(5, weights='distance')\n",
    "    else:\n",
    "        clf = KNeighborsClassifier(5, weights='distance')\n",
    "    trained_model = clf.fit(knn_x_train, knn_y_train)\n",
    "    imputed_values = trained_model.predict(knn_x_test)\n",
    "\n",
    "    # Объединяем спрогнозированные и остальные\n",
    "    # И собираем таблицу в исходное состояние\n",
    "    # Теперь у нас вместо пропущенных значений - спрогнозированные\n",
    "    X2 = np.hstack((knn_x_test, imputed_values.reshape(-1,1)))\n",
    "    X1 = np.hstack((knn_x_train, knn_y_train[:, np.newaxis]))\n",
    "    df_knn = np.vstack((X2, X1))\n",
    "\n",
    "    # Помещаем заполненный столбец в исходную таблицу\n",
    "    # Меняем тип данных на нужный\n",
    "    # И сортируем по индексу - Готово!\n",
    "    df_test[i] = df_knn[:,-1]\n",
    "\n",
    "df_test.sort_index(inplace=True)\n",
    "\n",
    "del df_knn, knn_x_train, knn_y_train, knn_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['ps_car_14']=df_train['ps_car_14'].apply(lambda x: x/1000000)\n",
    "df_train['ps_reg_03']=df_train['ps_reg_03'].apply(lambda x: x/1000000)\n",
    "\n",
    "df_test['ps_car_14']=df_test['ps_car_14'].apply(lambda x: x/1000000)\n",
    "df_test['ps_reg_03']=df_test['ps_reg_03'].apply(lambda x: x/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Разделение на категориальные и числовые \n",
    "num_cols = np.array(['ps_reg_01', 'ps_reg_02', 'ps_reg_03','ps_car_14', \n",
    "                     'ps_car_12', 'ps_car_13', \n",
    "                     'ps_calc_01','ps_calc_02','ps_calc_03'])\n",
    "cat_cols = np.array(list(set(df_test.columns.values.tolist()) - set(num_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Преобразование типов (после заполнения пропусков)\n",
    "for c in num_cols:\n",
    "    df_train[c] = df_train[c].astype(np.float32)\n",
    "    df_test[c] = df_test[c].astype(np.float32)\n",
    "    \n",
    "for c in cat_cols:\n",
    "    df_train[c] = df_train[c].astype(np.int8)\n",
    "    df_test[c] = df_test[c].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Оставляем хорошие признаки\n",
    "good_col=['ps_ind_03',\n",
    "'ps_car_13' ,           \n",
    "'ps_ind_05_cat',        \n",
    "'ps_reg_01',           \n",
    "'ps_ind_17_bin',        \n",
    "'ps_reg_02',           \n",
    "'ps_ind_15',            \n",
    "'ps_reg_03',           \n",
    "'ps_car_01_cat',        \n",
    "'ps_ind_06_bin',        \n",
    "'ps_ind_01',            \n",
    "'ps_car_14',           \n",
    "'ps_car_15',           \n",
    "'ps_ind_07_bin',        \n",
    "'ps_car_07_cat',        \n",
    "'ps_car_09_cat',       \n",
    "'ps_ind_02_cat',        \n",
    "'ps_car_04_cat',        \n",
    "'ps_car_12',            \n",
    "'ps_ind_16_bin',      \n",
    "'ps_car_11',           \n",
    "'ps_ind_09_bin',       \n",
    "'ps_car_02_cat',       \n",
    "'ps_car_06_cat',       \n",
    "'ps_ind_04_cat',        \n",
    "'ps_ind_08_bin',       \n",
    "'ps_car_03_cat',\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train[good_col + ['target']]\n",
    "df_test = df_test[good_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.to_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_test.to_pickle('/resources/data/driver/test_knn_dec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595206, 81) (892816, 81)\n"
     ]
    }
   ],
   "source": [
    "cat_features = [a for a in df_x_train.columns if a.endswith('cat')]\n",
    "\n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(df_x_train[column]), prefix=column)\n",
    "    df_x_train = pd.concat([df_x_train,temp],axis=1)\n",
    "    df_x_train = df_x_train.drop([column],axis=1)\n",
    "    \n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(df_test[column]), prefix=column)\n",
    "    df_test = pd.concat([df_test,temp],axis=1)\n",
    "    df_test = df_test.drop([column],axis=1)\n",
    "\n",
    "print(df_x_train.values.shape, df_test.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_x_train = df_train.drop('target', axis = 1)\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_pickle('/resources/data/driver/test_knn_dec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595206, 81) (892816, 81)\n"
     ]
    }
   ],
   "source": [
    "cat_features = [a for a in df_x_train.columns if a.endswith('cat')]\n",
    "\n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(df_x_train[column]), prefix=column)\n",
    "    df_x_train = pd.concat([df_x_train,temp],axis=1)\n",
    "    df_x_train = df_x_train.drop([column],axis=1)\n",
    "    \n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(df_test[column]), prefix=column)\n",
    "    df_test = pd.concat([df_test,temp],axis=1)\n",
    "    df_test = df_test.drop([column],axis=1)\n",
    "\n",
    "print(df_x_train.values.shape, df_test.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB_1 params\n",
    "xgb_2_params = {}\n",
    "xgb_2_params['max_depth'] = 4\n",
    "xgb_2_params['learning_rate'] = 0.1\n",
    "xgb_2_params['n_estimators'] = 400\n",
    "xgb_2_params['subsample'] = 0.8\n",
    "xgb_2_params['colsample_bytree'] = 0.8   \n",
    "xgb_2_params['min_child_weight'] = 6\n",
    "xgb_2_params['gamma']=10\n",
    "xgb_2_params['reg_alpha'] = 8\n",
    "xgb_2_params['reg_lambda'] = 1.3\n",
    "xgb_2_params['seed'] = 1\n",
    "xgb_2_params['n_jobs'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB_2 params\n",
    "xgb_2_params = {}\n",
    "xgb_2_params['max_depth'] = 4\n",
    "xgb_2_params['learning_rate'] = 0.1\n",
    "xgb_2_params['n_estimators'] = 400\n",
    "xgb_2_params['subsample'] = 0.8\n",
    "xgb_2_params['colsample_bytree'] = 0.8   \n",
    "xgb_2_params['min_child_weight'] = 6\n",
    "xgb_2_params['gamma']=10\n",
    "xgb_2_params['reg_alpha'] = 8\n",
    "xgb_2_params['reg_lambda'] = 1.3\n",
    "xgb_2_params['seed'] = 1\n",
    "xgb_2_params['n_jobs'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# scores_1.bin - СКОР РАБОЧЕГО ДАТАСЕТА ПРИ max_depth=4, n_estimator=110 без Oversample\n",
    "# with open('scores_1.bin', 'rb') as f:\n",
    "#      scores_1 = np.load(f)\n",
    "        \n",
    "scores_1 = np.array([])\n",
    "scores_2 = np.array([])\n",
    "\n",
    "# estimator = xgb.XGBClassifier(**xgb_1_params)\n",
    "#estimator = CatBoostClassifier(depth=6, iterations=500, random_seed=1)\n",
    "estimator = xgb.XGBClassifier(max_depth=4, n_estimator=110, seed=1, n_jobs=-1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "    for train_index, test_index in cv.split(df_x_train, df_y_train):\n",
    "\n",
    "        X_train, X_test = df_x_train.iloc[train_index, :], df_x_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = df_y_train.iloc[train_index], df_y_train.iloc[test_index]\n",
    "        \n",
    "        pos = pd.Series(Y_train == 1)\n",
    "        # Add positive examples\n",
    "        number = Y_train[Y_train==0].shape[0] // Y_train[Y_train==1].shape[0]\n",
    "        X_train = X_train.append([X_train.loc[pos]]*number)\n",
    "        Y_train = Y_train.append([Y_train.loc[pos]]*number)\n",
    "\n",
    "        #Shuffle data\n",
    "        idx = shuffle(np.arange(len(X_train)), random_state=i)\n",
    "        X_train = X_train.iloc[idx]\n",
    "        Y_train = Y_train.iloc[idx]\n",
    "\n",
    "\n",
    "        estimator.fit(X_train, Y_train)\n",
    "        pred = estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        gini_sc = GiniScore(Y_test, pred)\n",
    "        scores_1 = np.append(scores_1, gini_sc)  \n",
    "\n",
    "        \n",
    "#################################################\n",
    "### ИЗМЕНЕНИЯ ВО ВТОРОЙ МОДЕЛИ\n",
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_x_train = df_train.drop('target', axis = 1)\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_pickle('/resources/data/driver/test_knn_dec.pkl')\n",
    "#################################################\n",
    "#estimator = xgb.XGBClassifier(max_depth=4, n_estimator=110, seed=1, n_jobs=-1)\n",
    "#estimator = xgb.XGBClassifier(**xgb_2_params)\n",
    "#estimator = CatBoostClassifier(depth=7, iterations=500, random_seed=1)\n",
    "epoch = 0\n",
    "pred_xgb = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    for train_index, test_index in cv.split(df_x_train, df_y_train):\n",
    "        \n",
    "        epoch = epoch+1\n",
    "        \n",
    "        X_train, X_test = df_x_train.iloc[train_index, :], df_x_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = df_y_train.iloc[train_index], df_y_train.iloc[test_index]\n",
    "        \n",
    "        ### Oversampling\n",
    "        pos = pd.Series(Y_train == 1)\n",
    "        number = Y_train[Y_train==0].shape[0] // Y_train[Y_train==1].shape[0] // 5\n",
    "        X_train = X_train.append([X_train.loc[pos]]*number)\n",
    "        Y_train = Y_train.append([Y_train.loc[pos]]*number)\n",
    "        idx = shuffle(np.arange(len(X_train)), random_state=i)\n",
    "        X_train = X_train.iloc[idx]\n",
    "        Y_train = Y_train.iloc[idx]\n",
    "        ### Oversampling\n",
    "          \n",
    "       \n",
    "        estimator.fit(X_train, Y_train)\n",
    "        pred = estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        gini_sc = GiniScore(Y_test, pred)\n",
    "        scores_2 = np.append(scores_2, gini_sc)\n",
    "        print(gini_sc)\n",
    "        \n",
    "        pred_xgb['p_'+str(epoch)] = estimator.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================TARGET ENCODING==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_x_train = df_train.drop('target', axis = 1)\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_pickle('/resources/data/driver/test_knn_dec.pkl')\n",
    "\n",
    "###### Комбинации признаков\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    df_x_train[name1] = df_x_train[f1].apply(lambda x: str(x)) + \"_\" + df_x_train[f2].apply(lambda x: str(x))\n",
    "    df_test[name1] = df_test[f1].apply(lambda x: str(x)) + \"_\" + df_test[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df_x_train[name1].values) + list(df_test[name1].values))\n",
    "    df_x_train[name1] = lbl.transform(list(df_x_train[name1].values))\n",
    "    df_test[name1] = lbl.transform(list(df_test[name1].values))\n",
    "\n",
    "    \n",
    "####### Категориальные признаки для таргет кодирования\n",
    "f_cats = [f for f in df_x_train.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {}\n",
    "xgb_params['max_depth'] = 4\n",
    "xgb_params['learning_rate'] = 0.04\n",
    "# xgb_params['n_estimators'] = 400\n",
    "xgb_params['subsample'] = 0.8\n",
    "xgb_params['colsample_bytree'] = 0.8\n",
    "xgb_params['min_child_weight'] = 6\n",
    "xgb_params['gamma']=11\n",
    "xgb_params['reg_alpha'] = 11\n",
    "xgb_params['reg_lambda'] = 1.4\n",
    "xgb_params['seed'] = 1\n",
    "xgb_params['n_jobs'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.292496783736\n",
      "0.28771924788\n",
      "0.283662199193\n",
      "0.285594139112\n",
      "0.282534472093\n",
      "0.292711302101\n",
      "0.288531441956\n",
      "0.283141192201\n",
      "0.28500435866\n",
      "0.282611957544\n",
      "CPU times: user 1h 32min 47s, sys: 57.2 s, total: 1h 33min 44s\n",
      "Wall time: 15min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# OUTLIERS = False\n",
    "# scores_out = np.array([])\n",
    "\n",
    "SEEDS = 1\n",
    "SPLITS = 5\n",
    "\n",
    "scores_2 = np.array([])\n",
    "\n",
    "for param in [450, 500]:\n",
    "    \n",
    "    S_train = np.zeros((len(df_x_train), SEEDS))\n",
    "    S_test = np.zeros((len(df_test), SEEDS))   \n",
    "    #estimator = LGBMClassifier(**lgb_params)\n",
    "    #estimator = RGFClassifier(**rgf_params, learning_rate=param)\n",
    "    estimator = xgb.XGBClassifier(**xgb_params, n_estimators=param)\n",
    " \n",
    "    # сколько раз по фолдам\n",
    "    for seed in range(SEEDS):\n",
    "        \n",
    "        S_test_i = np.zeros((len(df_test), SPLITS))\n",
    "        cv = StratifiedKFold(n_splits=SPLITS, shuffle=True, random_state=seed)\n",
    "        for i, (train_index, test_index) in enumerate(cv.split(df_x_train, df_y_train)):\n",
    "           \n",
    "            X_train, X_test = df_x_train.iloc[train_index, :].copy(), df_x_train.iloc[test_index, :].copy()\n",
    "            Y_train, Y_test = df_y_train.iloc[train_index].copy(), df_y_train.iloc[test_index]\n",
    "            # Для таргет кодирования\n",
    "            X_sub = df_test.copy()\n",
    "\n",
    "            ### OVERSAMPLING\n",
    "            pos = pd.Series(Y_train == 1)\n",
    "            number = Y_train[Y_train==0].shape[0] // Y_train[Y_train==1].shape[0] // 5\n",
    "            X_train = X_train.append([X_train.loc[pos]]*number)\n",
    "            Y_train = Y_train.append([Y_train.loc[pos]]*number)\n",
    "            idx = shuffle(np.arange(len(X_train)), random_state=seed)\n",
    "            X_train = X_train.iloc[idx]\n",
    "            Y_train = Y_train.iloc[idx]\n",
    "            ### OVERSAMPLING\n",
    "\n",
    "            ### TARGET ENCODING\n",
    "            for f in f_cats:\n",
    "                X_train[f + \"_avg\"], X_test[f + \"_avg\"], X_sub[f + \"_avg\"] = target_encode(\n",
    "                                                                trn_series=X_train[f],\n",
    "                                                                val_series=X_test[f],\n",
    "                                                                tst_series=X_sub[f],\n",
    "                                                                target=Y_train,\n",
    "                                                                min_samples_leaf=200,\n",
    "                                                                smoothing=15,\n",
    "                                                                noise_level=0)\n",
    "            ### TARGET ENCODING    \n",
    "            \n",
    "            estimator.fit(X_train, Y_train)\n",
    "            pred = estimator.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "#             if OUTLIERS:\n",
    "                \n",
    "#                 gini_sc = GiniScore(Y_test, pred)\n",
    "#                 scores_2 = np.append(scores_2, gini_sc)\n",
    "#                 print(gini_sc)\n",
    "                \n",
    "#                 ############################# OUTLIERS REMOVE\n",
    "#                 pred_class = estimator.predict(X_test)\n",
    "            \n",
    "#                 outliers = pd.DataFrame(index=Y_test.index, columns=['act', 'class', 'pred'])\n",
    "#                 outliers['act'] = Y_test\n",
    "#                 outliers['class'] = pred_class\n",
    "#                 outliers['pred'] = pred\n",
    "\n",
    "#                 # Ошибочные 1 (убираем выбросы)\n",
    "#                 outliers = outliers.drop(outliers[(outliers['act'] < outliers['class']) & (outliers['pred'] > \n",
    "#                                 outliers['pred'][outliers['act'] < outliers['class']].quantile(0.95))].index)\n",
    "\n",
    "#                 # Ошибочные 0 (убираем выбросы)\n",
    "#                 outliers = outliers.drop(outliers[(outliers['act'] > outliers['class']) & (outliers['pred'] < \n",
    "#                                 outliers['pred'][outliers['act'] > outliers['class']].quantile(0.05))].index)\n",
    "\n",
    "#                 gini_sc = GiniScore(outliers['act'], outliers['pred'])\n",
    "#                 scores_out = np.append(scores_out, gini_sc)\n",
    "#                 print(gini_sc)\n",
    "#                 del outliers\n",
    "#                 ############################### OUTLIERS REMOVE          \n",
    "\n",
    "\n",
    "            gini_sc = GiniScore(Y_test, pred)\n",
    "            scores_2 = np.append(scores_2, gini_sc)\n",
    "            print(gini_sc)\n",
    "\n",
    "#             S_train[test_index, seed] = pred\n",
    "#             S_test_i[:, i] = estimator.predict_proba(X_sub)[:, 1]\n",
    "\n",
    "            del X_test, X_train, Y_train, X_sub\n",
    "        \n",
    "#         S_test[:, seed] = np.mean(S_test_i, axis=1)\n",
    "\n",
    "#     S_train = np.mean(S_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28369792499976493"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.286401368403\n",
      "0.286400050492\n"
     ]
    }
   ],
   "source": [
    "# Сравниваем\n",
    "# with open('scores_xgb_trees.bin', 'rb') as f:\n",
    "#     scores_1 = np.load(f)\n",
    "\n",
    "for i in range(0, len(scores_2), 5):\n",
    "    print(np.mean(scores_2[i:i+5]))\n",
    "\n",
    "#     print(ttest_rel(scores_2[i:i+30], scores_2[150:180]))\n",
    "#     print(np.mean(scores_2[120:150]), np.mean(scores_2[150:180]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняем\n",
    "with open('stack_xgb_trees_300_600.bin', 'wb') as f:\n",
    "    np.save(f, scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Среднее теста\n",
    "pred_xgb['target'] = np.exp(np.mean(pred_xgb.applymap(lambda x: np.log(x)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(892816, 2)\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv('/resources/data/driver/sample_submission.csv').sort_values('id') \n",
    "df_submission['target'] = final\n",
    "df_submission.to_csv('/resources/data/driver/submission.csv', index=False)\n",
    "print(df_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняем стек\n",
    "with open('stack_xgb_trees_300_600.bin', 'wb') as f:\n",
    "    np.save(f, scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_x_train = df_train.drop('target', axis = 1)\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_pickle('/resources/data/driver/test_knn_dec.pkl')\n",
    "\n",
    "###### Комбинации признаков\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    df_x_train[name1] = df_x_train[f1].apply(lambda x: str(x)) + \"_\" + df_x_train[f2].apply(lambda x: str(x))\n",
    "    df_test[name1] = df_test[f1].apply(lambda x: str(x)) + \"_\" + df_test[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df_x_train[name1].values) + list(df_test[name1].values))\n",
    "    df_x_train[name1] = lbl.transform(list(df_x_train[name1].values))\n",
    "    df_test[name1] = lbl.transform(list(df_test[name1].values))\n",
    "\n",
    "    \n",
    "####### Категориальные признаки для таргет кодирования\n",
    "f_cats = [f for f in df_x_train.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.286263\n",
    "cat_params1 = {}\n",
    "cat_params1['depth'] = 7\n",
    "cat_params1['learning_rate'] = 0.02\n",
    "cat_params1['rsm'] = 0.8\n",
    "cat_params1['iterations'] = 700\n",
    "cat_params1['l2_leaf_reg'] = 18\n",
    "cat_params1['random_seed'] = 1\n",
    "\n",
    "# 0.286199\n",
    "cat_params2 = {}\n",
    "cat_params2['depth'] = 7\n",
    "cat_params2['learning_rate'] = 0.02\n",
    "cat_params2['rsm'] = 0.8\n",
    "cat_params2['iterations'] = 750\n",
    "cat_params2['l2_leaf_reg'] = 22\n",
    "cat_params2['random_seed'] = 1\n",
    "\n",
    "# 0.287700\n",
    "lgb_params1 = {}\n",
    "lgb_params1['n_estimators'] = 1000\n",
    "lgb_params1['learning_rate'] = 0.02\n",
    "lgb_params1['num_leaves'] = 16\n",
    "lgb_params1['subsample'] = 0.7\n",
    "lgb_params1['subsample_freq'] = 1\n",
    "lgb_params1['colsample_bytree'] = 0.7\n",
    "lgb_params1['reg_alpha'] = 18\n",
    "lgb_params1['reg_lambda'] = 1.6\n",
    "lgb_params1['random_state'] = 1\n",
    "\n",
    "# 0.287167\n",
    "lgb_params3 = {}\n",
    "lgb_params3['n_estimators'] = 1150\n",
    "lgb_params3['learning_rate'] = 0.02\n",
    "lgb_params3['num_leaves'] = 20\n",
    "lgb_params3['subsample'] = 0.7\n",
    "lgb_params3['subsample_freq'] = 1\n",
    "lgb_params3['colsample_bytree'] = 0.7\n",
    "lgb_params3['reg_alpha'] = 18\n",
    "lgb_params3['reg_lambda'] = 1.8\n",
    "lgb_params3['random_state'] = 1\n",
    "\n",
    "xgb_params1 = {}\n",
    "xgb_params1['max_depth'] = 4\n",
    "xgb_params1['learning_rate'] = 0.04\n",
    "xgb_params1['n_estimators'] = 450\n",
    "xgb_params1['subsample'] = 0.8\n",
    "xgb_params1['colsample_bytree'] = 0.8\n",
    "xgb_params1['min_child_weight'] = 6\n",
    "xgb_params1['gamma']=11\n",
    "xgb_params1['reg_alpha'] = 11\n",
    "xgb_params1['reg_lambda'] = 1.4\n",
    "xgb_params1['seed'] = 1\n",
    "xgb_params1['n_jobs'] = -1\n",
    "\n",
    "xgb_params2 = {}\n",
    "xgb_params2['max_depth'] = 4\n",
    "xgb_params2['learning_rate'] = 0.07\n",
    "xgb_params2['n_estimators'] = 250\n",
    "xgb_params2['subsample'] = 0.8\n",
    "xgb_params2['colsample_bytree'] = 0.8\n",
    "xgb_params2['min_child_weight'] = 6\n",
    "xgb_params2['gamma']=10\n",
    "xgb_params2['reg_alpha'] = 8\n",
    "xgb_params2['reg_lambda'] = 1.3\n",
    "xgb_params2['seed'] = 1\n",
    "xgb_params2['n_jobs'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params1 = {}\n",
    "xgb_params1['max_depth'] = 4\n",
    "xgb_params1['learning_rate'] = 0.04\n",
    "xgb_params1['n_estimators'] = 450\n",
    "xgb_params1['subsample'] = 0.8\n",
    "xgb_params1['colsample_bytree'] = 0.8\n",
    "xgb_params1['min_child_weight'] = 6\n",
    "xgb_params1['gamma']=11\n",
    "xgb_params1['reg_alpha'] = 11\n",
    "xgb_params1['reg_lambda'] = 1.4\n",
    "xgb_params1['seed'] = 1\n",
    "xgb_params1['n_jobs'] = -1\n",
    "\n",
    "xgb_params2 = {}\n",
    "xgb_params2['max_depth'] = 4\n",
    "xgb_params2['learning_rate'] = 0.07\n",
    "xgb_params2['n_estimators'] = 250\n",
    "xgb_params2['subsample'] = 0.8\n",
    "xgb_params2['colsample_bytree'] = 0.8\n",
    "xgb_params2['min_child_weight'] = 6\n",
    "xgb_params2['gamma']=10\n",
    "xgb_params2['reg_alpha'] = 8\n",
    "xgb_params2['reg_lambda'] = 1.3\n",
    "xgb_params2['seed'] = 1\n",
    "xgb_params2['n_jobs'] = -1\n",
    "\n",
    "xgb_params3 = {}\n",
    "xgb_params3['max_depth'] = 4\n",
    "xgb_params3['learning_rate'] = 0.03\n",
    "xgb_params3['n_estimators'] = 550\n",
    "xgb_params3['subsample'] = 0.8\n",
    "xgb_params3['colsample_bytree'] = 0.8\n",
    "xgb_params3['min_child_weight'] = 6\n",
    "xgb_params3['gamma'] = 13\n",
    "xgb_params3['reg_alpha'] = 11\n",
    "xgb_params3['reg_lambda'] = 1.5\n",
    "xgb_params3['seed'] = 1\n",
    "xgb_params3['n_jobs'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.287700\n",
    "lgb_params1 = {}\n",
    "lgb_params1['n_estimators'] = 1000\n",
    "lgb_params1['learning_rate'] = 0.02\n",
    "lgb_params1['num_leaves'] = 16\n",
    "lgb_params1['subsample'] = 0.7\n",
    "lgb_params1['subsample_freq'] = 1\n",
    "lgb_params1['colsample_bytree'] = 0.7\n",
    "lgb_params1['reg_alpha'] = 18\n",
    "lgb_params1['reg_lambda'] = 1.6\n",
    "lgb_params1['random_state'] = 1\n",
    "\n",
    "# 0.286942\n",
    "lgb_params2 = {}\n",
    "lgb_params2['n_estimators'] = 1100\n",
    "lgb_params2['learning_rate'] = 0.02\n",
    "lgb_params2['num_leaves'] = 20\n",
    "lgb_params2['subsample'] = 0.7\n",
    "lgb_params2['subsample_freq'] = 1\n",
    "lgb_params2['colsample_bytree'] = 0.7\n",
    "lgb_params2['reg_alpha'] = 19\n",
    "lgb_params2['reg_lambda'] = 1.7\n",
    "lgb_params2['random_state'] = 1\n",
    "\n",
    "# 0.287167\n",
    "lgb_params3 = {}\n",
    "lgb_params3['n_estimators'] = 1150\n",
    "lgb_params3['learning_rate'] = 0.02\n",
    "lgb_params3['num_leaves'] = 20\n",
    "lgb_params3['subsample'] = 0.7\n",
    "lgb_params3['subsample_freq'] = 1\n",
    "lgb_params3['colsample_bytree'] = 0.7\n",
    "lgb_params3['reg_alpha'] = 18\n",
    "lgb_params3['reg_lambda'] = 1.8\n",
    "lgb_params3['random_state'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.286263\n",
    "cat_params1 = {}\n",
    "cat_params1['depth'] = 7\n",
    "cat_params1['learning_rate'] = 0.02\n",
    "cat_params1['rsm'] = 0.8\n",
    "cat_params1['iterations'] = 700\n",
    "cat_params1['l2_leaf_reg'] = 18\n",
    "cat_params1['random_seed'] = 1\n",
    "\n",
    "# 0.286199\n",
    "cat_params2 = {}\n",
    "cat_params2['depth'] = 7\n",
    "cat_params2['learning_rate'] = 0.02\n",
    "cat_params2['rsm'] = 0.8\n",
    "cat_params2['iterations'] = 750\n",
    "cat_params2['l2_leaf_reg'] = 22\n",
    "cat_params2['random_seed'] = 1\n",
    "\n",
    "# 0.285511\n",
    "cat_params3 = {}\n",
    "cat_params3['depth'] = 7\n",
    "cat_params3['learning_rate'] = 0.03\n",
    "cat_params3['rsm'] = 0.7\n",
    "cat_params3['iterations'] = 500\n",
    "cat_params3['bagging_temperature'] = 1\n",
    "cat_params3['l2_leaf_reg'] = 15\n",
    "cat_params3['random_seed'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.292496783736\n",
      "0.28771924788\n",
      "0.283662199193\n",
      "0.285594139112\n",
      "0.282534472093\n",
      "0.286401368403\n",
      "0.291430328185\n",
      "0.286239361502\n",
      "0.280513404216\n",
      "0.285975330709\n",
      "0.282835860454\n",
      "0.285398857014\n",
      "0.292182987965\n",
      "0.287575204218\n",
      "0.283047680548\n",
      "0.2865257133\n",
      "0.28318801078\n",
      "0.286503919362\n",
      "CPU times: user 2h 20min 55s, sys: 1min 55s, total: 2h 22min 50s\n",
      "Wall time: 34min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# OUTLIERS = False\n",
    "# scores_out = np.array([])\n",
    "\n",
    "# model1 = xgb.XGBClassifier(**xgb_params1)\n",
    "# model2 = xgb.XGBClassifier(**xgb_params2)\n",
    "# model3 = xgb.XGBClassifier(**xgb_params3)\n",
    "\n",
    "# model1 = CatBoostClassifier(**cat_params1)\n",
    "# model2 = CatBoostClassifier(**cat_params2)\n",
    "# model3 = CatBoostClassifier(**cat_params3)\n",
    "\n",
    "model1 = LGBMClassifier(**lgb_params1)\n",
    "model2 = LGBMClassifier(**lgb_params3)\n",
    "model3 = xgb.XGBClassifier(**xgb_params1)\n",
    "model4 = xgb.XGBClassifier(**xgb_params2)\n",
    "model5 = CatBoostClassifier(**cat_params1)\n",
    "model6 = CatBoostClassifier(**cat_params2)\n",
    "\n",
    "models = (model1, model2, model3, model4, model5, model6)\n",
    "\n",
    "SPLITS = 5\n",
    "\n",
    "S_train = np.zeros((len(df_x_train), len(models)))\n",
    "S_test = np.zeros((len(df_test), len(models)))   \n",
    " \n",
    "# сколько раз по фолдам\n",
    "for j, clf in enumerate(models):\n",
    "\n",
    "    estimator = clf\n",
    "    S_test_i = np.zeros((len(df_test), SPLITS))\n",
    "    cv = StratifiedKFold(n_splits=SPLITS, shuffle=True, random_state=0)\n",
    "    scores_2 = np.array([])\n",
    "    \n",
    "    # Для каждой модели\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(df_x_train, df_y_train)):\n",
    "           \n",
    "        X_train, X_test = df_x_train.iloc[train_index, :].copy(), df_x_train.iloc[test_index, :].copy()\n",
    "        Y_train, Y_test = df_y_train.iloc[train_index].copy(), df_y_train.iloc[test_index]\n",
    "        # Для таргет кодирования\n",
    "        X_sub = df_test.copy()\n",
    "\n",
    "        ### OVERSAMPLING\n",
    "        pos = pd.Series(Y_train == 1)\n",
    "        number = Y_train[Y_train==0].shape[0] // Y_train[Y_train==1].shape[0] // 5\n",
    "        X_train = X_train.append([X_train.loc[pos]]*number)\n",
    "        Y_train = Y_train.append([Y_train.loc[pos]]*number)\n",
    "        idx = shuffle(np.arange(len(X_train)), random_state=0)\n",
    "        X_train = X_train.iloc[idx]\n",
    "        Y_train = Y_train.iloc[idx]\n",
    "        ### OVERSAMPLING\n",
    "\n",
    "        ### TARGET ENCODING\n",
    "        for f in f_cats:\n",
    "            X_train[f + \"_avg\"], X_test[f + \"_avg\"], X_sub[f + \"_avg\"] = target_encode(\n",
    "                                                                trn_series=X_train[f],\n",
    "                                                                val_series=X_test[f],\n",
    "                                                                tst_series=X_sub[f],\n",
    "                                                                target=Y_train,\n",
    "                                                                min_samples_leaf=200,\n",
    "                                                                smoothing=15,\n",
    "                                                                noise_level=0)\n",
    "        ### TARGET ENCODING    \n",
    "            \n",
    "        estimator.fit(X_train, Y_train)\n",
    "        pred = estimator.predict_proba(X_test)[:, 1]\n",
    "     \n",
    "\n",
    "\n",
    "        gini_sc = GiniScore(Y_test, pred)\n",
    "        scores_2 = np.append(scores_2, gini_sc)\n",
    "        print(gini_sc)\n",
    "\n",
    "        S_train[test_index, j] = pred\n",
    "        S_test_i[:, i] = estimator.predict_proba(X_sub)[:, 1]\n",
    "\n",
    "        del X_test, X_train, Y_train, X_sub\n",
    "        \n",
    "    S_test[:, j] = np.exp(np.mean(np.log(S_test_i), axis=1))\n",
    "    print(np.mean(scores_2))\n",
    "# Мы получили три столбца предикта на трэйн и тест\n",
    "\n",
    "# Предикт (не надо для второго уровня)\n",
    "stacker = LogisticRegression()\n",
    "stacker.fit(S_train, df_y_train)\n",
    "final = stacker.predict_proba(S_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13729029,  0.13452692,  0.13357566],\n",
       "       [ 0.14481544,  0.14039931,  0.14026862],\n",
       "       [ 0.13018696,  0.13070774,  0.13242098],\n",
       "       [ 0.0806234 ,  0.08367767,  0.08156432],\n",
       "       [ 0.17964633,  0.18165595,  0.17904538]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# три столбца полученные на стеке моделей \n",
    "val_train = pd.DataFrame(index=df_x_train.index, columns=['1', '2', '3'])\n",
    "val_train['1'] = S_train[:, 0]\n",
    "val_train['2'] = S_train[:, 1]\n",
    "val_train['3'] = S_train[:, 2]\n",
    "\n",
    "val_test = pd.DataFrame(index=df_test.index, columns=['1', '2', '3'])\n",
    "val_test['1'] = S_test[:, 0]\n",
    "val_test['2'] = S_test[:, 1]\n",
    "val_test['3'] = S_test[:, 2]\n",
    "\n",
    "val_train.to_csv('/resources/data/driver/res_cat_train.csv', index=False)\n",
    "val_test.to_csv('/resources/data/driver/res_cat_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595206, 29)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.137290</td>\n",
       "      <td>0.134527</td>\n",
       "      <td>0.133576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144815</td>\n",
       "      <td>0.140399</td>\n",
       "      <td>0.140269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.130187</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>0.132421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080623</td>\n",
       "      <td>0.083678</td>\n",
       "      <td>0.081564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179646</td>\n",
       "      <td>0.181656</td>\n",
       "      <td>0.179045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3\n",
       "id                              \n",
       "0   0.137290  0.134527  0.133576\n",
       "1   0.144815  0.140399  0.140269\n",
       "2   0.130187  0.130708  0.132421\n",
       "3   0.080623  0.083678  0.081564\n",
       "4   0.179646  0.181656  0.179045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from rgf.sklearn import RGFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Коэффициент Gini\n",
    "def GiniScore(y_actual, y_pred):\n",
    "    return 2*roc_auc_score(y_actual, y_pred)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_train = pd.read_csv('/resources/data/driver/res_lgb_train.csv')\n",
    "val_test = pd.read_csv('/resources/data/driver/res_lgb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_train = pd.read_csv('/resources/data/driver/res_xgb_train.csv')\n",
    "val_test = pd.read_csv('/resources/data/driver/res_xgb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_train = pd.read_csv('/resources/data/driver/res_cat_train.csv')\n",
    "val_test = pd.read_csv('/resources/data/driver/res_cat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "df_y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.292814099727\n",
      "0.284917599661\n",
      "0.276468064751\n",
      "0.281757271965\n",
      "0.297751463232\n"
     ]
    }
   ],
   "source": [
    "# Собираем предикт для каждого\n",
    "SPLITS = 5\n",
    "scores_2 = np.array([])\n",
    "\n",
    "S_train = np.zeros((len(val_train), 1))\n",
    "S_test = np.zeros((len(val_test), 1)) \n",
    "S_test_i = np.zeros((len(val_test), SPLITS))\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=SPLITS, shuffle=True, random_state=1)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv.split(val_train, df_y_train)):\n",
    "           \n",
    "    X_train, X_test = val_train.iloc[train_index, :], val_train.iloc[test_index, :]\n",
    "    Y_train, Y_test = df_y_train.iloc[train_index], df_y_train.iloc[test_index]\n",
    "\n",
    "\n",
    "    ### OVERSAMPLING\n",
    "    pos = pd.Series(Y_train == 1)\n",
    "    number = Y_train[Y_train==0].shape[0] // Y_train[Y_train==1].shape[0] // 5\n",
    "    X_train = X_train.append([X_train.loc[pos]]*number)\n",
    "    Y_train = Y_train.append([Y_train.loc[pos]]*number)\n",
    "    idx = shuffle(np.arange(len(X_train)), random_state=1)\n",
    "    X_train = X_train.iloc[idx]\n",
    "    Y_train = Y_train.iloc[idx]\n",
    "    ### OVERSAMPLING\n",
    " \n",
    "            \n",
    "    estimator.fit(X_train, Y_train)\n",
    "    pred = estimator.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "\n",
    "    gini_sc = GiniScore(Y_test, pred)\n",
    "    scores_2 = np.append(scores_2, gini_sc)\n",
    "    print(gini_sc)\n",
    "\n",
    "    S_train[test_index, 0] = pred\n",
    "    S_test_i[:, i] = estimator.predict_proba(val_test)[:, 1]\n",
    "\n",
    "    del X_test, X_train, Y_train\n",
    "        \n",
    "S_test[:,0] = np.exp(np.mean(np.log(S_test_i), axis=1))\n",
    "\n",
    "# Сохраняем\n",
    "with open('stack_train_3.bin', 'wb') as f:\n",
    "    np.save(f, S_train[:,0])\n",
    "with open('stack_test_3.bin', 'wb') as f:\n",
    "    np.save(f, S_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21853407,  0.16603845,  0.13581187, ...,  0.1083093 ,\n",
       "        0.14366838,  0.1130128 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_x_train = df_train.drop('target', axis = 1)\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "df_y_train.reset_index\n",
    "df_test = pd.read_pickle('/resources/data/driver/test_knn_dec.pkl')\n",
    "\n",
    "## Для трех моделей##############\n",
    "S_train_f = np.zeros((len(df_x_train), 3))\n",
    "S_test_f = np.zeros((len(df_test), 3)) \n",
    "\n",
    "for i in range(3):\n",
    "    # train\n",
    "    with open('stack_train_'+str(i+1)+'.bin', 'rb') as f:\n",
    "        S_train_f[:,i] = np.load(f)\n",
    "    # test\n",
    "    with open('stack_test_'+str(i+1)+'.bin', 'rb') as f:\n",
    "        S_test_f[:,i] = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13698623,  0.13428062,  0.13738251],\n",
       "       [ 0.1326837 ,  0.13403583,  0.14237263],\n",
       "       [ 0.12606727,  0.1311661 ,  0.13450562],\n",
       "       ..., \n",
       "       [ 0.17128715,  0.17324402,  0.17677975],\n",
       "       [ 0.12396437,  0.1256496 ,  0.12768861],\n",
       "       [ 0.15664125,  0.15590674,  0.15544927]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacker = LogisticRegression()\n",
    "stacker.fit(S_train_f, df_y_train.values)\n",
    "final = stacker.predict_proba(S_test_f)[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02710455,  0.02641555,  0.02584769, ...,  0.03225679,\n",
       "        0.02554802,  0.0301071 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=1).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models))) # Заготовка для предиктов на трейне\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models))) # Заготовка для предиктов на тесте\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n",
    "        print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict_proba(S_test)[:,1]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('/resources/data/driver/train_knn_dec.pkl')\n",
    "df_x_train = df_train.drop('target', axis = 1)\n",
    "df_y_train = df_train['target']    \n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_pickle('/resources/data/driver/test_knn_dec.pkl')\n",
    "\n",
    "###### Комбинации признаков\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    df_x_train[name1] = df_x_train[f1].apply(lambda x: str(x)) + \"_\" + df_x_train[f2].apply(lambda x: str(x))\n",
    "    df_test[name1] = df_test[f1].apply(lambda x: str(x)) + \"_\" + df_test[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df_x_train[name1].values) + list(df_test[name1].values))\n",
    "    df_x_train[name1] = lbl.transform(list(df_x_train[name1].values))\n",
    "    df_test[name1] = lbl.transform(list(df_test[name1].values))\n",
    "\n",
    "    \n",
    "####### Категориальные признаки для таргет кодирования\n",
    "f_cats = [f for f in df_x_train.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.286263\n",
    "cat_params1 = {}\n",
    "cat_params1['depth'] = 7\n",
    "cat_params1['learning_rate'] = 0.02\n",
    "cat_params1['rsm'] = 0.8\n",
    "cat_params1['iterations'] = 700\n",
    "cat_params1['l2_leaf_reg'] = 18\n",
    "cat_params1['random_seed'] = 1\n",
    "\n",
    "# 0.286199\n",
    "cat_params2 = {}\n",
    "cat_params2['depth'] = 7\n",
    "cat_params2['learning_rate'] = 0.02\n",
    "cat_params2['rsm'] = 0.8\n",
    "cat_params2['iterations'] = 750\n",
    "cat_params2['l2_leaf_reg'] = 22\n",
    "cat_params2['random_seed'] = 1\n",
    "\n",
    "# 0.287700\n",
    "lgb_params1 = {}\n",
    "lgb_params1['n_estimators'] = 1000\n",
    "lgb_params1['learning_rate'] = 0.02\n",
    "lgb_params1['num_leaves'] = 16\n",
    "lgb_params1['subsample'] = 0.7\n",
    "lgb_params1['subsample_freq'] = 1\n",
    "lgb_params1['colsample_bytree'] = 0.7\n",
    "lgb_params1['reg_alpha'] = 18\n",
    "lgb_params1['reg_lambda'] = 1.6\n",
    "lgb_params1['random_state'] = 1\n",
    "\n",
    "# 0.287167\n",
    "lgb_params3 = {}\n",
    "lgb_params3['n_estimators'] = 1150\n",
    "lgb_params3['learning_rate'] = 0.02\n",
    "lgb_params3['num_leaves'] = 20\n",
    "lgb_params3['subsample'] = 0.7\n",
    "lgb_params3['subsample_freq'] = 1\n",
    "lgb_params3['colsample_bytree'] = 0.7\n",
    "lgb_params3['reg_alpha'] = 18\n",
    "lgb_params3['reg_lambda'] = 1.8\n",
    "lgb_params3['random_state'] = 1\n",
    "\n",
    "xgb_params1 = {}\n",
    "xgb_params1['max_depth'] = 4\n",
    "xgb_params1['learning_rate'] = 0.04\n",
    "xgb_params1['n_estimators'] = 450\n",
    "xgb_params1['subsample'] = 0.8\n",
    "xgb_params1['colsample_bytree'] = 0.8\n",
    "xgb_params1['min_child_weight'] = 6\n",
    "xgb_params1['gamma']=11\n",
    "xgb_params1['reg_alpha'] = 11\n",
    "xgb_params1['reg_lambda'] = 1.4\n",
    "xgb_params1['seed'] = 1\n",
    "xgb_params1['n_jobs'] = -1\n",
    "\n",
    "xgb_params2 = {}\n",
    "xgb_params2['max_depth'] = 4\n",
    "xgb_params2['learning_rate'] = 0.07\n",
    "xgb_params2['n_estimators'] = 250\n",
    "xgb_params2['subsample'] = 0.8\n",
    "xgb_params2['colsample_bytree'] = 0.8\n",
    "xgb_params2['min_child_weight'] = 6\n",
    "xgb_params2['gamma']=10\n",
    "xgb_params2['reg_alpha'] = 8\n",
    "xgb_params2['reg_lambda'] = 1.3\n",
    "xgb_params2['seed'] = 1\n",
    "xgb_params2['n_jobs'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LGBMClassifier(**lgb_params1)\n",
    "model2 = LGBMClassifier(**lgb_params3)\n",
    "model3 = xgb.XGBClassifier(**xgb_params1)\n",
    "model4 = xgb.XGBClassifier(**xgb_params2)\n",
    "model5 = CatBoostClassifier(**cat_params1)\n",
    "model6 = CatBoostClassifier(**cat_params2)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "       \n",
    "stack = Ensemble(n_splits=5,\n",
    "        stacker = log_model,\n",
    "        base_models = (model1, model2, model3, model4, model5, model6))        \n",
    "        \n",
    "y_pred = stack.fit_predict(df_x_train, df_y_train, df_test)        \n",
    "\n",
    "sub_1 = pd.DataFrame()\n",
    "sub_1['id'] = id_test\n",
    "sub_1['target'] = y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
