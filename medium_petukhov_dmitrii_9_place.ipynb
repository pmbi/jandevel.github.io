{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import gensim\n",
    "from gensim.matutils  import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "train_target = pd.read_csv('train_log1p_recommends.csv', index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def preprocess(path_to_inp_json_file):\n",
    "    content_list = []\n",
    "    images_list = []\n",
    "    frames_list = []\n",
    "    published_list = []\n",
    "    author_list = []\n",
    "    domain_list = []\n",
    "    tags_list = []\n",
    "    lang_list = []\n",
    "\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file:\n",
    "        for line in tqdm_notebook(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ').replace('\\xa0', ' ').replace('\\u200a', ' ')\n",
    "\n",
    "            tags_str = []\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            try:\n",
    "                tag_block = soup.find('ul', class_='tags')\n",
    "                tags = tag_block.find_all('a')\n",
    "                for tag in tags:\n",
    "                    tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n",
    "                tags = ' '.join(tags_str)\n",
    "            except Exception:\n",
    "                tags = 'none'\n",
    "            tags_list.append(tags)\n",
    "        \n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            content_list.append(content_no_html_tags)\n",
    "\n",
    "            frames = re.findall(r'iframeContainer', content)\n",
    "            images = re.findall(r'<img class=', content)\n",
    "\n",
    "            images_list.append(len(images))\n",
    "            frames_list.append(len(frames))\n",
    "            \n",
    "            published = json_data['published']['$date']\n",
    "            published_list.append(published) \n",
    " \n",
    "            author = json_data['meta_tags']['author'].strip()\n",
    "            author_list.append(author) \n",
    "        \n",
    "            domain = json_data['domain']\n",
    "            domain_list.append(domain)\n",
    "            \n",
    "            try:\n",
    "                lang = detect(content_no_html_tags)\n",
    "            except Exception:      \n",
    "                lang = 'undefined'   \n",
    "            lang_list.append(lang)\n",
    "            \n",
    "\n",
    "    return content_list, images_list, frames_list, published_list, author_list, domain_list, tags_list, lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing train data...')\n",
    "content_train, images_list, frames_list, published_list, author_list, domain_list, tags_list, lang_list = preprocess(path_to_inp_json_file='train.json')\n",
    "df_train['images'] = images_list\n",
    "df_train['frames'] = frames_list\n",
    "df_train['published'] = published_list\n",
    "df_train['author'] = author_list\n",
    "df_train['domain'] = domain_list\n",
    "df_train['tags'] = tags_list\n",
    "df_train['lang'] = lang_list\n",
    "\n",
    "print('Preprocessing test data...')\n",
    "content_test, images_list, frames_list, published_list, author_list, domain_list, tags_list, lang_list = preprocess(path_to_inp_json_file='test.json')\n",
    "df_test['images'] = images_list\n",
    "df_test['frames'] = frames_list\n",
    "df_test['published'] = published_list\n",
    "df_test['author'] = author_list\n",
    "df_test['domain'] = domain_list\n",
    "df_test['tags'] = tags_list\n",
    "df_test['lang'] = lang_list\n",
    "\n",
    "del images_list, frames_list, published_list, author_list, domain_list, tags_list, lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['num_tags'] = df_train['tags'].apply(lambda x: len(x.split()))\n",
    "df_test['num_tags'] = df_test['tags'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.to_pickle('df_train.pkl')\n",
    "df_test.to_pickle('df_test.pkl')\n",
    "with open('content_train.pkl', 'wb') as f:\n",
    "    pickle.dump(content_train, f)\n",
    "with open('content_test.pkl', 'wb') as f:\n",
    "    pickle.dump(content_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('df_train.pkl')\n",
    "df_test = pd.read_pickle('df_test.pkl')\n",
    "with open ('content_train.pkl', 'rb') as f:\n",
    "    content_train = pickle.load(f)\n",
    "with open ('content_test.pkl', 'rb') as f:\n",
    "    content_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['published'] = pd.to_datetime(df_train['published'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "df_test['published'] = pd.to_datetime(df_test['published'], format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing LDA features...\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing LDA features...')\n",
    "cv = CountVectorizer(max_features=10000, min_df = 0.1, max_df = 0.8)\n",
    "sparse_train = cv.fit_transform(content_train)\n",
    "sparse_test  = cv.transform(content_test)\n",
    "full_sparse_data =  vstack([sparse_train, sparse_test])\n",
    "\n",
    "corpus_data_gensim = gensim.matutils.Sparse2Corpus(full_sparse_data, documents_columns=False)\n",
    "del sparse_train, sparse_test, full_sparse_data\n",
    "\n",
    "vocabulary_gensim = {}\n",
    "for key, val in cv.vocabulary_.items():\n",
    "    vocabulary_gensim[val] = key\n",
    "    \n",
    "dict = Dictionary()\n",
    "dict.merge_with(vocabulary_gensim)\n",
    "\n",
    "lda = LdaModel(corpus_data_gensim, num_topics = 30 )\n",
    "\n",
    "def document_to_lda_features(lda_model, document):\n",
    "    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n",
    "    topic_importances = np.array(topic_importances)\n",
    "    return topic_importances[:,1]\n",
    "\n",
    "lda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))\n",
    "data_pd_lda_features = pd.DataFrame(lda_features)\n",
    "\n",
    "df_lda_train = data_pd_lda_features.iloc[:len(y_train), :]\n",
    "df_lda_test = data_pd_lda_features.iloc[len(y_train):, :]\n",
    "del data_pd_lda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing features...\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing features...')\n",
    "df_train = pd.concat([df_train, df_lda_train], axis=1)\n",
    "df_test = pd.concat([df_test, df_lda_test.reset_index(drop=True)], axis=1)\n",
    "del df_lda_train, df_lda_test\n",
    "\n",
    "df_train['content'] = content_train\n",
    "df_train['target'] = y_train\n",
    "\n",
    "df_train.sort_values(by='published', inplace=True)\n",
    "df_train = df_train[df_train.published>='2016-01-01']\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "idx = len(df_train[df_train.published<'2017-04-01'])\n",
    "\n",
    "content_train = df_train['content'].values.tolist()\n",
    "y_train = df_train['target'].values\n",
    "df_train.drop(['content', 'target', 'tags'], axis=1, inplace=True)\n",
    "\n",
    "df_test.drop(['tags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN feats: (45938, 30681)\n",
      "TEST feats: (34645, 30681)\n",
      "dummies: (80583, 30681)\n"
     ]
    }
   ],
   "source": [
    "def a_d(row):\n",
    "    return str(row['domain'])+'_'+str(row['author'])\n",
    "\n",
    "idx_split = len(df_train)\n",
    "df_full = pd.concat([df_train, df_test])\n",
    "\n",
    "\n",
    "# Временные признаки\n",
    "df_full['dow'] = df_full['published'].apply(lambda x: x.dayofweek)\n",
    "df_full['month'] = df_full['published'].apply(lambda x: x.month)\n",
    "df_full['hour'] = df_full['published'].apply(lambda x: x.hour)\n",
    "df_full['year'] = df_full['published'].apply(lambda x: x.year)\n",
    "df_full['year_month'] = df_full['published'].apply(lambda x: 100 * x.year + x.month)\n",
    "\n",
    "df_full['hour_sin'] = np.sin((df_full.hour)*(2.*np.pi/24))\n",
    "df_full['hour_cos'] = np.cos((df_full.hour)*(2.*np.pi/24))\n",
    "\n",
    "df_full['dow_sin'] = np.sin((df_full.dow)*(2.*np.pi/7))\n",
    "df_full['dow_cos'] = np.cos((df_full.dow)*(2.*np.pi/7))\n",
    "\n",
    "df_full['month_sin'] = np.sin((df_full.month)*(2.*np.pi/12))\n",
    "df_full['month_cos'] = np.cos((df_full.month)*(2.*np.pi/12))\n",
    "\n",
    "#Другие\n",
    "counts = df_full.author.value_counts()\n",
    "repl = counts[counts < 2].index\n",
    "df_full['author'] = df_full.author.replace(repl, 'other').values\n",
    "\n",
    "counts = df_full.domain.value_counts()\n",
    "repl = counts[counts < 1000].index\n",
    "df_full['domain'] = df_full.domain.replace(repl, 'other').values\n",
    "\n",
    "df_full['a_d'] = df_full.apply(a_d, axis=1)\n",
    "\n",
    "# Преобразование\n",
    "list_to_dums = ['author', 'domain', 'lang', 'year', 'year_month', 'num_tags', 'images', 'a_d', 'frames']\n",
    "dummies = pd.get_dummies(df_full, columns = list_to_dums, #drop_first=True,\n",
    "                            prefix=['col_{}'.format(i) for i in range(len(list_to_dums))], sparse=False)\n",
    "\n",
    "# Удаляем ненужные\n",
    "list_to_drop = ['published', 'hour', 'dow', 'month']\n",
    "dummies.drop(list_to_drop, axis=1, inplace=True)\n",
    "\n",
    "X_train_feats = dummies.iloc[:idx_split, :]\n",
    "X_test_feats = dummies.iloc[idx_split:, :]\n",
    "\n",
    "print('TRAIN feats: {}'.format(X_train_feats.shape))\n",
    "print('TEST feats: {}'.format(X_test_feats.shape))\n",
    "print('dummies: {}'.format(dummies.shape))\n",
    "del dummies, df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample='sample_submission.csv'):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making TF-IDF features...\n",
      "Training Ridge model...\n"
     ]
    }
   ],
   "source": [
    "print('Making TF-IDF features...')\n",
    "tfidf_content_params={'ngram_range': (1,3),\n",
    "                      'max_features': 150000, \n",
    "                      'stop_words': 'english',\n",
    "                      'sublinear_tf': True\n",
    "                      }\n",
    "tfidf_content = TfidfVectorizer(**tfidf_content_params)\n",
    "\n",
    "ridge_params={\n",
    "               'alpha': 1.30,\n",
    "               'random_state': 1,\n",
    "                  }\n",
    "\n",
    "ridge = Ridge(**ridge_params)\n",
    "\n",
    "X_train_content = tfidf_content.fit_transform(content_train)\n",
    "X_test_content = tfidf_content.transform(content_test)\n",
    "\n",
    "X_train_csr = csr_matrix(hstack([X_train_content, X_train_feats.values])) \n",
    "X_test_csr = csr_matrix(hstack([X_test_content, X_test_feats.values]))  \n",
    "\n",
    "print('Training Ridge model...')\n",
    "ridge_pred = ridge.fit(X_train_csr, y_train).predict(X_test_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n"
     ]
    }
   ],
   "source": [
    "print('Training LightGBM model...')\n",
    "params = {\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'objective': 'mean_absolute_error',\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'metric': 'mean_absolute_error',\n",
    "    'bagging_seed': 1,\n",
    "    'num_threads': 8\n",
    "}\n",
    "\n",
    "np.random.seed(1)\n",
    "lgb_train = lgb.Dataset(X_train_csr, label=y_train)\n",
    "bst_lgb = lgb.train(params, lgb_train, num_boost_round=600)\n",
    "lgb_pred = bst_lgb.predict(X_test_csr, num_iteration=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction...\n"
     ]
    }
   ],
   "source": [
    "print('Making prediction...')\n",
    "mix_pred = 0.733 * ridge_pred + (1-0.733) * lgb_pred\n",
    "shift = 4.33328 - np.mean(mix_pred)\n",
    "mix_pred = np.log1p(np.round(np.expm1(mix_pred)))\n",
    "mix_pred = mix_pred + shift\n",
    "write_submission_file(mix_pred, 'petukhov_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
